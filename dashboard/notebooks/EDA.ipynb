{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def load_credentials():\n",
    "    \"\"\"\n",
    "    Load Supabase credentials from .env file.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"access_key_id\": os.getenv(\"SUPABASE_ACCESS_KEY_ID\"),\n",
    "        \"secret_access_key\": os.getenv(\"SUPABASE_SECRET_ACCESS_KEY\"),\n",
    "        \"endpoint_url\": os.getenv(\"SUPABASE_ENDPOINT\"),\n",
    "        \"region_name\": os.getenv(\"SUPABASE_REGION\"),\n",
    "        \"bucket_name\": os.getenv(\"SUPABASE_BUCKET_NAME\"),\n",
    "    }\n",
    "\n",
    "def read_and_merge_csv_from_supabase_s3(subfolders, credentials):\n",
    "    \"\"\"\n",
    "    Read CSV files from specified subfolders in a Supabase S3-compatible storage.\n",
    "    Outer join all CSV files in each subfolder and return a DataFrame per subfolder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    subfolders : list\n",
    "        List of subfolders to search for CSV files.\n",
    "    credentials : dict\n",
    "        Dictionary containing Supabase S3 credentials.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A list of DataFrames, one per subfolder.\n",
    "    \"\"\"\n",
    "    # Create S3 client with custom endpoint\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=credentials[\"access_key_id\"],\n",
    "        aws_secret_access_key=credentials[\"secret_access_key\"],\n",
    "        endpoint_url=credentials[\"endpoint_url\"],\n",
    "        region_name=credentials[\"region_name\"],\n",
    "        config=boto3.session.Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        merged_df = pd.DataFrame()\n",
    "        try:\n",
    "            paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "            for page in paginator.paginate(Bucket=credentials[\"bucket_name\"], Prefix=f\"{subfolder}/\"):\n",
    "                if \"Contents\" not in page:\n",
    "                    continue\n",
    "                for obj in page[\"Contents\"]:\n",
    "                    if obj[\"Key\"].lower().endswith(\".csv\"):\n",
    "                        try:\n",
    "                            # Fetch the CSV file\n",
    "                            response = s3_client.get_object(Bucket=credentials[\"bucket_name\"], Key=obj[\"Key\"])\n",
    "                            csv_content = response[\"Body\"].read()\n",
    "                            df = pd.read_csv(io.BytesIO(csv_content))\n",
    "\n",
    "                            # Outer join with the merged DataFrame\n",
    "                            if merged_df.empty:\n",
    "                                merged_df = df\n",
    "                            else:\n",
    "                                merged_df = pd.merge(merged_df, df, how=\"outer\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing file {obj['Key']}: {e}\")\n",
    "            dataframes.append(merged_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subfolder {subfolder}: {e}\")\n",
    "            dataframes.append(pd.DataFrame())  # Add an empty DataFrame in case of error for consistency\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load credentials from .env\n",
    "    credentials = load_credentials()\n",
    "    \n",
    "    # Define subfolders\n",
    "    subfolders = [\"raw/auchan\", \"raw/pingo_doce\", \"raw/continente\"]\n",
    "    \n",
    "    # Read and merge data\n",
    "    merged_dataframes = read_and_merge_csv_from_supabase_s3(subfolders, credentials)\n",
    "    \n",
    "    return merged_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file raw/continente/20241113/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241115/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241116/bebe.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241116/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241120/bebe.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241120/biologicos.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241120/frescos.csv: An error occurred while reading from response stream: ('Connection broken: IncompleteRead(86304 bytes read, 131906 more expected)', IncompleteRead(86304 bytes read, 131906 more expected))\n",
      "Error processing file raw/continente/20241120/higiene-beleza.csv: Connection was closed before we received a valid response from endpoint URL: \"https://ahluezrirjxhplqwvspy.supabase.co/storage/v1/s3/retail/raw/continente/20241120/higiene-beleza.csv\".\n",
      "Error processing file raw/continente/20241121/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241122/bebe.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241122/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241122/frescos.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241123/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241124/bebe.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241124/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241124/congelados.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241125/bebe.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241125/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241125/congelados.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241125/mercearias.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241126/bebe.csv: You are trying to merge on object and int64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241126/biologicos.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n",
      "Error processing file raw/continente/20241126/congelados.csv: You are trying to merge on object and float64 columns for key 'Product ID'. If you wish to proceed you should use pd.concat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[        product_id  ... timestamp\n",
       " 0          3074118  ...  20241113\n",
       " 1           293295  ...  20241113\n",
       " 2          2878683  ...  20241113\n",
       " 3          2736681  ...  20241113\n",
       " 4          3009129  ...  20241113\n",
       " ...            ...  ...       ...\n",
       " 283659      763248  ...  20241126\n",
       " 283660     3771505  ...  20241126\n",
       " 283661     3771462  ...  20241126\n",
       " 283662     3830541  ...  20241126\n",
       " 283663     3771503  ...  20241126\n",
       " \n",
       " [283664 rows x 13 columns],\n",
       "                                              product_id  ...        timestamp\n",
       " 0                                 agua-pingo-doce-33-cl  ...  20241113_173400\n",
       " 1                                 agua-pingo-doce-50-cl  ...  20241113_173400\n",
       " 2                   iced-tea-pessego-pingo-doce-6x20-cl  ...  20241113_173400\n",
       " 3                         agua-com-gas-pingo-doce-25-cl  ...  20241113_173400\n",
       " 4                   nectar-de-8-frutos-pingo-doce-20-cl  ...  20241113_173400\n",
       " ...                                                 ...  ...              ...\n",
       " 6202  pizza-fresca-de-queijo-e-fiambre-sem-gluten-e-...  ...  20241126_090645\n",
       " 6203        rosca-de-presunto-e-queijo-pingo-doce-450-g  ...  20241126_090645\n",
       " 6204                   lasanha-bolonhesa-pingo-doce-1kg  ...  20241126_090645\n",
       " 6205                cannelloni-bolonhesa-pingo-doce-1kg  ...  20241126_090645\n",
       " 6206  canelloni-de-espinafres-com-ricotta-pingo-doce...  ...  20241126_090645\n",
       " \n",
       " [6207 rows x 8 columns],\n",
       "                                       Product Name  ...      source\n",
       " 0                       Cadeira de Refeição Branca  ...  Continente\n",
       " 1              Box Fraldas Activity XXL 11-16kg T5  ...  Continente\n",
       " 2                 Papa Infantil Farinha Láctea +6M  ...  Continente\n",
       " 3                         Toalhitas Bebé Aqua Pure  ...  Continente\n",
       " 4                                   Nestum Mel +3A  ...  Continente\n",
       " ...                                            ...  ...         ...\n",
       " 36139              Cápsulas de Café Intenso Int 10  ...  Continente\n",
       " 36140                 Milho Brasil Canjica-Cachupa  ...  Continente\n",
       " 36141  Condimento Azeite Virgem Extra Aroma Fumado  ...  Continente\n",
       " 36142         Sortido de Sticks de Chocolate Belga  ...  Continente\n",
       " 36143                       Molho Chutney de Manga  ...  Continente\n",
       " \n",
       " [36144 rows x 12 columns]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = main()\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auchan, pingo_doce, continente = dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
