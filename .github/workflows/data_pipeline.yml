# .github/workflows/data_pipeline.yml
name: Data Pipeline 2 (Single Job with Monitoring)

on:
  workflow_dispatch: # Allows manual triggering from GitHub UI
  schedule:
    - cron: '0 0 * * *' # Runs daily at midnight UTC

# Environment variables available to the job
env:
  # --- Monitoring Parameters ---
  COLLECTION_DURATION_SECONDS: 300 # <<< Adjust: How long to monitor (e.g., 600 for 10 min)
  SAMPLE_INTERVAL_SECONDS: 10    # <<< Adjust: How often to sample stats
  # B2 details for monitoring upload defined in secrets below

jobs:
  run-pipeline-and-monitor:
    runs-on: ubuntu-latest
    permissions:
      contents: write # For checkout and potentially committing results if needed

    # Define secrets needed by the job
    # secrets: inherit # Alternative: if secrets are defined at org/repo level for all workflows

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: sql-integration

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10.6'

      - name: Install system dependencies and AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y libpq-dev awscli # Install both system deps and awscli

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r continente_price_tracker/requirements.txt

      - name: Create .env file
        run: |
          echo "DATABASE_NAME=${{ secrets.DATABASE_NAME }}" >> continente_price_tracker/.env
          echo "USER=${{ secrets.USER }}" >> continente_price_tracker/.env
          echo "PASSWORD=${{ secrets.PASSWORD }}" >> continente_price_tracker/.env
          echo "HOST=${{ secrets.HOST }}" >> continente_price_tracker/.env
          echo "SUPABASE_ACCESS_KEY_ID=${{ secrets.SUPABASE_ACCESS_KEY_ID }}" >> continente_price_tracker/.env
          echo "SUPABASE_SECRET_ACCESS_KEY=${{ secrets.SUPABASE_SECRET_ACCESS_KEY }}" >> continente_price_tracker/.env
          echo "SUPABASE_ENDPOINT=${{ secrets.SUPABASE_ENDPOINT }}" >> continente_price_tracker/.env
          echo "SUPABASE_REGION=${{ secrets.SUPABASE_REGION }}" >> continente_price_tracker/.env
          echo "SUPABASE_BUCKET_NAME=${{ secrets.SUPABASE_BUCKET_NAME }}" >> continente_price_tracker/.env
          # B2 creds for pipeline scripts (if needed) AND for monitoring upload
          echo "B2_KEY_ID=${{ secrets.B2_KEY_ID }}" >> continente_price_tracker/.env
          echo "B2_PASSWORD=${{ secrets.B2_PASSWORD }}" >> continente_price_tracker/.env
          echo "B2_BUCKET=${{ secrets.B2_BUCKET }}" >> continente_price_tracker/.env
          # Add endpoint only if python scripts need it via .env
          # echo "B2_ENDPOINT_URL=${{ secrets.B2_ENDPOINT_URL }}" >> continente_price_tracker/.env
        # Note: Monitoring script reads B2 creds via aws configure step below

      - name: Configure AWS CLI for Backblaze B2 (for monitoring upload)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.B2_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.B2_PASSWORD }}
        run: |
          echo "Configuring AWS CLI for B2 uploads..."
          aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID"
          aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"

      - name: Run Pipeline in Background & Monitor in Foreground
        id: run_and_monitor
        # Pass B2 details needed specifically for the upload part via env for this step
        env:
          B2_BUCKET_NAME: ${{ secrets.B2_BUCKET }}
          B2_ENDPOINT_URL: ${{ secrets.B2_ENDPOINT_URL }} # Needed for upload later
          MAIN_WORKFLOW_RUN_ID: ${{ github.run_id }}
          # GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # If python scripts need it
        run: |
          echo "Starting main data pipeline script(s) in background..."
          # Execute the pipeline logic. If the first script fails, subsequent ones won't run.
          # Runs within a subshell `()` so `wait` works correctly.
          (python continente_price_tracker/src/object_storage/main_concurrency.py && \
           python continente_price_tracker/src/db/upload_from_b2.py && \
           python continente_price_tracker/src/db/augmentation_main.py) &

          # Capture the Process ID (PID) of the background pipeline process
          PIPELINE_PID=$!
          echo "Pipeline running in background with PID: $PIPELINE_PID"

          echo "Starting monitoring script in foreground..."
          # Run the monitoring script. It will collect stats for COLLECTION_DURATION_SECONDS
          # The script reads duration/interval/run_id from top-level env vars
          # It outputs results to stats_output.json
          python - <<EOF > stats_output.json
          import subprocess, json, time, datetime, statistics, collections, re, os

          DURATION = int(os.getenv('COLLECTION_DURATION_SECONDS', 300))
          INTERVAL = int(os.getenv('SAMPLE_INTERVAL_SECONDS', 10))
          MAIN_RUN_ID = os.getenv('MAIN_WORKFLOW_RUN_ID', 'unknown_run')
          NUM_SAMPLES = DURATION // INTERVAL

          container_data = collections.defaultdict(lambda: {'cpu_readings': [], 'mem_readings_mib': [], 'timestamps': []})
          start_time = time.time()
          end_time = start_time + DURATION
          actual_samples = 0

          print(f"Monitoring Run ID: {MAIN_RUN_ID}")
          print(f"Collecting docker stats for {DURATION} seconds, sampling every {INTERVAL} seconds (max {NUM_SAMPLES} samples).")

          while time.time() < end_time:
              loop_start_time = time.time()
              current_sample_time = datetime.datetime.utcnow().isoformat() + "Z"
              actual_samples += 1
              try:
                  result = subprocess.run(
                      ['docker', 'stats', '--no-stream', '--format', '{{json .}}'],
                      capture_output=True, text=True, check=True, timeout=INTERVAL - 1
                  )
                  for line in result.stdout.strip().split('\n'):
                      if not line: continue
                      try:
                          stats = json.loads(line)
                          container_id = stats.get("ID") or stats.get("Container")
                          if not container_id: continue
                          cpu_perc = float(stats.get("CPUPerc", "0.0%").replace('%', ''))
                          mem_usage_str = stats.get("MemUsage", "0MiB / 0MiB").split('/')[0].strip()
                          mem_mib = 0.0
                          match = re.match(r"([\d\.]+)([a-zA-Z]+)", mem_usage_str)
                          if match:
                              value = float(match.group(1)); unit = match.group(2).lower()
                              if unit == 'kib': mem_mib = value / 1024.0
                              elif unit == 'mib': mem_mib = value
                              elif unit == 'gib': mem_mib = value * 1024.0
                              elif unit == 'tib': mem_mib = value * 1024.0 * 1024.0
                              elif unit == 'b': mem_mib = value / (1024.0 * 1024.0)
                          container_data[container_id]['name'] = stats.get("Name", "unknown")
                          container_data[container_id]['cpu_readings'].append(cpu_perc)
                          container_data[container_id]['mem_readings_mib'].append(mem_mib)
                          container_data[container_id]['timestamps'].append(current_sample_time)
                      except Exception as e: print(f"Warning: Error processing line '{line}': {e}")
              except subprocess.TimeoutExpired: print("Warning: 'docker stats' command timed out.")
              except subprocess.CalledProcessError as e:
                  if "Cannot connect" in e.stderr or "Is the docker daemon running?" in e.stderr: print("Error: Docker daemon not running."); break
                  elif not container_data: print("Info: No running containers or 'docker stats' empty.")
                  else: print(f"Warning: 'docker stats' failed: {e.stderr}")
              except Exception as e: print(f"Unexpected error during collection: {e}")
              elapsed_in_loop = time.time() - loop_start_time
              sleep_time = max(0, INTERVAL - elapsed_in_loop)
              if time.time() + sleep_time < end_time: time.sleep(sleep_time)
              else: break
          collection_end_time = time.time()
          print(f"Collected {actual_samples} samples over {collection_end_time - start_time:.2f} seconds.")
          final_stats = {
              "monitoring_target_run_id": MAIN_RUN_ID,
              "collection_start_utc": datetime.datetime.utcfromtimestamp(start_time).isoformat() + "Z",
              "collection_end_utc": datetime.datetime.utcfromtimestamp(collection_end_time).isoformat() + "Z",
              "collection_duration_seconds": round(collection_end_time - start_time, 2),
              "requested_duration_seconds": DURATION, "sample_interval_seconds": INTERVAL,
              "samples_taken": actual_samples, "containers": []
          }
          for cid, data in container_data.items():
              if not data['cpu_readings']: continue
              cpu, mem = data['cpu_readings'], data['mem_readings_mib']
              final_stats["containers"].append({
                  "container_id": cid, "container_name": data['name'], "samples_collected": len(cpu),
                  "cpu_percentage": {"max": round(max(cpu),2) if cpu else 0, "median": round(statistics.median(cpu),2) if cpu else 0, "average": round(statistics.mean(cpu),2) if cpu else 0, "min": round(min(cpu),2) if cpu else 0},
                  "memory_mib": {"max": round(max(mem),2) if mem else 0, "median": round(statistics.median(mem),2) if mem else 0, "average": round(statistics.mean(mem),2) if mem else 0, "min": round(min(mem),2) if mem else 0},
              })
          print(json.dumps(final_stats, indent=2))
          EOF
          echo "Monitoring script finished. Stats saved to stats_output.json"

          # Now, wait for the background pipeline process to finish
          echo "Waiting for background pipeline process (PID: $PIPELINE_PID) to complete..."
          wait $PIPELINE_PID
          PIPELINE_EXIT_CODE=$? # Capture the exit code of the background process

          echo "Background pipeline process finished with exit code: $PIPELINE_EXIT_CODE"

          # Check if the pipeline failed
          if [ $PIPELINE_EXIT_CODE -ne 0 ]; then
            echo "Error: Background pipeline process failed with exit code $PIPELINE_EXIT_CODE."
            exit $PIPELINE_EXIT_CODE # Fail this entire step with the same exit code
          fi

          echo "Pipeline process completed successfully."
          # Set an output flag if needed for subsequent steps
          echo "pipeline_completed=true" >> $GITHUB_OUTPUT

      - name: Upload Monitoring Stats to Backblaze B2
        # This step only runs if the previous step (run_and_monitor) succeeded
        env:
           # These are needed again specifically for the aws s3 cp command
           AWS_ENDPOINT_URL: ${{ secrets.B2_ENDPOINT_URL }}
           B2_BUCKET_NAME: ${{ secrets.B2_BUCKET }}
           MAIN_WORKFLOW_RUN_ID: ${{ github.run_id }}
        run: |
          OUTPUT_FILE="stats_output.json"
          if [ ! -s $OUTPUT_FILE ]; then
            echo "Warning: Stats output file is empty or does not exist. Skipping upload."
            exit 0 # Exit successfully, maybe monitoring failed but pipeline was ok
          fi

          CURRENT_DATE=$(date -u +'%Y-%m-%d')
          TARGET_DIRECTORY="logs/infra/${CURRENT_DATE}"
          DETAILED_TIMESTAMP=$(date -u +"%Y%m%dT%H%M%SZ")
          TARGET_FILENAME="run-${MAIN_WORKFLOW_RUN_ID}-${DETAILED_TIMESTAMP}-docker-stats.json"
          TARGET_S3_PATH="s3://${B2_BUCKET_NAME}/${TARGET_DIRECTORY}/${TARGET_FILENAME}"

          echo "Uploading $OUTPUT_FILE to $TARGET_S3_PATH"
          aws s3 cp "$OUTPUT_FILE" "$TARGET_S3_PATH" --endpoint-url "$AWS_ENDPOINT_URL"

          if [ $? -eq 0 ]; then
            echo "Monitoring stats upload successful."
          else
            echo "Error: Monitoring stats upload failed."
            # Decide if upload failure should fail the whole job
            # exit 1 # Uncomment to fail the job if upload fails
          fi
